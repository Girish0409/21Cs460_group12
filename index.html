<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <title>Machine Leanrning : Group 12</title>
        <link href="http://www.niser.ac.in/~smishra/css/smlab.css" rel="stylesheet" type="text/css" />
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
    </head>

    <body>
        <body style="background-color: rgb(187, 177, 233);">
        <div class="container">
            <h1>Machine Learning CS460 : Group 12</h1>
            <p>Course Instructor : Dr. Subhankar Mishra</p>
            
            <hr />
            <h2>Group Contributers</h2>
            <p>
                Group Repository : <a href="https://github.com/Girish0409/21Cs460_group12">Link</a>
               <ul>
                   <li>Girish Tripathy <br> 
                    1811072, School of Physical Sciences, NISER <br>
                    <a href="https://github.com/girish0409">Github</a>                    
                
                   </li>

                   <li>Shashank Saumya <br> 
                    1811143, School of Physical Sciences, NISER <br>
                    <a href="https://github.com/shashanksaumya123">Github</a>                  
                
                   </li>
               </ul> 
            </p>
            <hr />

            <h1>Title of the project : <br>
                Multiclass classification of White blood cells using Convolutional Neural Network (CNN)<br>
                
            </h1>
            <hr />

            <h3>Biological Background of the project</h3>
            <p>
                WBCs (White Blood cells) also known as leukocytes are the cells of immune system that are involved in protecting the body against both
                infectious invadors and foreign diseases. <br>
                Depending upon the presence of small structures known as granules, WBCs are classified into two different types:
                <ul>
                    <li>Granulocytes: Presence of Granular cytoplasm</li>
                    <li>Agranulocytes: Absence of Granular cytoplasm</li>
                </ul>
                Depending upon the type of nuclus, Granulocytes and Agranulocytes are furthur divided into different types <br>
                <br>
                Granulocytes:
                <ul> 
                    <li>Basophil: About 0.4% in adults. Responsible for allergic and antigen response by releasing a chemical called histamine. Since they are rarest of the EBC and share similarities between other types of cell, they are difficult to study, hence we dont have any data of the same</li>
                    <li>Eosinophil: About 2.3% in adults. Can be distinguished as Bi-lobed nucleus. It rises in responce to allergies, parasitic infections and disease of the spleen and central nervous systems. <br>
                        <centre><img src="./images/eo.png" width="400" height="300" alt="Eosinophil"></centre>
                    </li>
                    <li>
                        Neutrophil: About 60-70% in adults. Can be distinguished as Multilobed nucleus. Responsible for protection against bacterial or fungal infections.<br> 
                        <centre><img src="./images/nu.png" width="400" height="300" alt="Neutrophil"></centre> 
                    </li>
                    
                </ul>
                <br>
                Agranulocytes:
                <ul>
                    <li>
                        Monocyte: About 5.3% in adults. Can be distinguished as the largest nucleus. Responsible for phagocytoses of used up neutrophils.<br>
                        <centre><img src="./images/mono.png" width="400" height="300" alt="Monocyte"></centre>
                    </li>
                    <li>
                        Lymphocyte: About 30% in adults. Can be distinguished as eccentric nucleus.<br>
                        <centre><img src="./images/lym.png" width="400" height="300" alt="Lymphocyte"></centre>
                    </li>
                </ul>

            </p>

            <h3>
                Motivation and plans :
            </h3>

            <p>
                There is a need to quickly identify blood cells at bulk in the medical field. We propose using a CNN to build a machine which can classify the stained cells to the type of WBCs. The reason we use CNN is because it is known for its high accuracy when it comes to image recognition.
                
            </p>

            <p>
                We plan to first get the RGB info on each pixel of the image. The stained cells are of a different colour from the other cells so it would be easier to identify the pixels which contain the WBC under inspection.
                Then we will use a CNN and train it to get a model which will be able to classify an image containing a stained WBC. 
              
            </p>

            <h3>KYD : Know Your Dataset</h3>
            <p>
                               
                The dataset we will using is <a href="https://www.kaggle.com/paultimothymooney/blood-cells">WBC Classification dataset</a> uploaded on Kaggle by Paul Mooney. <br>
                The database consist of about 10,000 images of White blood cells, divided into 4 different classes. Each class have about 2500 RGB images of dimention 320 x 240.  
                
            </p>

            <hr />
            <h3>Midway plans and Work Division :</h3>
            Our Midway plans are follows
            <ul>
                <li>Learn about neural networks from various sources</li>
                <li>Cleaning our data and Preprocessing</li>
                <li>Have a simple CNN model trained and ready to predict</li>
            </ul>
            
            Post midway, we will be exploring methods and layers to make our algorithm and model better in terms of accuracy.

            <br>
            Work Division :
            <ul>
                <li>Refinement and preparing the dataset : Shashank (Image to data squeezing, reshaping), Girish (Finding the Optimal way to work with the data)</li>
                <li>Planning and coding the model :- Girish + Shashank</li>                
                <li>Report Writing, presentation and Website maintainance : Girish</li>
            </ul>

            <hr />

            <h1>Midway Works :</h1> 
            <hr />
            <p>
                Up till now we have done the following:
            </p>
            
            <ul>
                <li>Learnt about the theory behind NN and CNN</li>
                <li>Read the relevant papers</li>
                <li>Done some experimentation of various dataset including ours</li>
            </ul>  
            
            <hr />
             <h2>What is a NN and CNN?</h2>
             <p>
                 <img src="./images/nn.png" alt="Neural Network"> <br>
                 Neural Network (NN) also known as Artificial Neural Network (ANN) as the name suggest is a network of neurons or nodes and forms the basis of Machine Learning and heart of the deep learning algorithms. <br>
                 It mimics a network of biological neurons. All the different neurons or nodes are connected to each other by weights giving a layer liked structure. A positive weight corresponds to excitory connection and a negative weight corresponds to inhibitory connections. <br>
                 This weights can be found out or calculated during the traning of the machine learning model. <br>
                 The first layer of a NN is known as input layer, the last layer is known as output layer. The layers in between both, is known as hidden layer. Number of hidden layers is decided considering the dataset and complexity of the model. <br>
                 Backpropagation is an important algorithm by which a neural network learns from its own mistakes. <br>
                 <br>
                 There are various types of Neural networks like Artificial neural network (ANN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), Long short term memory neural network (LSTM) etc. 
                 Each type of Neural network have its own uses. <br> 
                 <br>
                 Neural Network that is frequently used for image classification is Convolutional Neural network (CNN). CNN is a type of NN with another hidden layer known as "convolution layer". <br>
                 A convolution layer have multiple filters, each filter tried to find a particular pattern. Lets say we are trying to classify an image into birds. A convolution layer will have multiple filters, like a beak filter which detect pattern in beaks of the bird, or a eye filter which detects eye of a bird. These filters or nodes will activate nodes of another convolution layer which might have a head filter or a body filter. When all the required filters or nodes get activated, the output layer will gives us the type of bird we are trying to classify bases on the input of the hidden layers.<br>
                 <img src="./images/filter.png" alt="Filters"> <br>
                 <br>

                 We will be using Sequencial method from Keras API in tensorflow library to build our model. We will also be using other functions and method for preprocessing of the image and furthur tweaking of the model. <br> <br>

                The CNN model we have, have different Conv2D layers, pooling layers and Dense layers. Number of each layer is a hyperparameter we can tune and test. Our basic CNN is illustrated by the image below <br>
                <img src="./images/cnn.png"  alt="Basic CNN Model">

                 

                
             </p>

             <hr />
             <h2>Some Analysis of the papers</h2>
             <hr />
             <h3>Paper 1</h3>
             <img src="./images/lenet.png" alt="LeNet">
             <ul>
                 <li>Important points and analysis on Paper 1: WBC Classification using CNN, Mayank sharma, Aishwarya Bhave and Rekh Ram Jaghel
                    <ul>
                        <li>Use the same WBC data (with some augmentation and downsizing)</li>
                        <li>Use a 7 layered (LeNet) CNN model</li>
                        <li>Tried for different epochs and Learning rates</li>
                        <li>Best results at 20 epochs and 0.001 Learning rate (0.8793 Accuracy)</li>
                    </ul>
                 </li>
                    
                 <li>
                    Important points and analysis on Paper 2: Counting and Classification of WBC using ANN, Shubham Manik, lalit Mohan Saini and Nikhil Vadera
                    <ul>
                        <li>They have the image of a slide which contains multiple WBCs</li>
                        <li>Used Image Segmentation Techniques likes Cell and Nucleus Segmentation</li>
                        <li>Morphological features are extracted like, Cell area, nucleus area, area ratio, number of lobules in nucleus, etc</li>
                        <li>Use NN to perform multiclass Classification</li>
                        <li>However, they only had 90 leukocyte (WBC) samples which is very small data</li>
                        <li>The accuracy was around 98.9%</li>
                    </ul>
                     
                 </li>
             </ul>

            <hr />

            <h3>Various experiments we have done. <a href="https://github.com/Girish0409/21cs460_group12/tree/main/Code%20and%20Data">Link </a>to the codes</h3>
            <table style="margin-left: auto; margin-right: auto;">
                <tbody>
                <tr>
                <td>&nbsp;<strong>Experiment No.</strong></td>
                <td><strong>Dataset</strong></td>
                <td><strong>Model</strong>&nbsp;</td>
                <td><strong>Optimiser</strong>&nbsp;</td>
                <td><strong>&nbsp;Activation Function</strong></td>
                <td><strong>&nbsp;Training Accuracy</strong></td>
                <td><strong>Testing Accuracy</strong></td>
                </tr>
                <tr>
                <td>&nbsp;1</td>
                <td>Mnist</td>
                <td>&nbsp;NN Single Layered</td>
                <td>&nbsp;Adam</td>
                <td>&nbsp;Sigmoid</td>
                <td>&nbsp;98 %</td>
                <td>&nbsp;93 %</td>
                </tr>
                <tr>
                <td>&nbsp;2</td>
                <td>Mnist</td>
                <td>&nbsp;NN Multi Layered</td>
                <td>Adam&nbsp;</td>
                <td>&nbsp;Sigmoid</td>
                <td>&nbsp;99 %</td>
                <td>&nbsp;98%</td>
                </tr>
                <tr>
                <td>&nbsp;3</td>
                <td>Cifar10</td>
                <td>&nbsp;NN Multi Layered</td>
                <td>SGD&nbsp;</td>
                <td>Sigmoid&nbsp;</td>
                <td>&nbsp;99 %</td>
                <td>&nbsp;55 %</td>
                </tr>
                <tr>
                <td>&nbsp;4</td>
                <td>Cifar10</td>
                <td>&nbsp;NN Multi Layered</td>
                <td>Adam&nbsp;</td>
                <td>Softmax&nbsp;</td>
                <td>&nbsp;98 %</td>
                <td>&nbsp;48 %</td>
                </tr>
                <tr>
                <td>&nbsp;5</td>
                <td>Cifar10</td>
                <td>&nbsp;NN Multi Layered</td>
                <td>&nbsp;SGD</td>
                <td>Softmax&nbsp;</td>
                <td>&nbsp;98%</td>
                <td>&nbsp;56 %</td>
                </tr>
                <tr>
                <td>&nbsp;6</td>
                <td>Cifar10</td>
                <td>&nbsp;CNN Multi layered</td>
                <td>Adam&nbsp;</td>
                <td>Softmax&nbsp;</td>
                <td>&nbsp;93 %</td>
                <td>&nbsp;68%</td>
                </tr>
                <tr>
                <td>&nbsp;7</td>
                <td>WBC</td>
                <td>CNN Multilayered&nbsp;</td>
                <td>Adam&nbsp;</td>
                <td>Softmax&nbsp;</td>
                <td>100 %&nbsp;</td>
                <td>&nbsp;48%</td>
                </tr>
                </tbody>
                </table>
                
            <hr />

            <h3>Future ideas and comments on the current experiments</h3>
            <ul>
                <li>Understand LeNet and if possible come up with a better model/layer combination</li>
                <li>Our CNN model is getting overfitted on the databases. This maybe due lack of data augmentation. To tackle that, we are thinking to do the following
                    <ul>
                        <li>Use data augmentation, resizing etc.</li>
                        <li>Use image segmentation method to separate the nucleus from the image, and train the model on that</li>
                        <li>Use GridSearchCV for tuning of hyperparameters</li>
                    </ul>
                </li>
            </ul>

            <hr />
            <h1>Final Works</h1>
            <hr />
            <p>
                Till now we are able to do the following:
                <ul>
                    <li>Generated data from image augmentation, but could not train our model on the same due to Out of memory errors.</li>
                    <li>Did image segmentation techniques to extract the nucleus from the image</li>
                    <li>Tried out regularisation techniques to tackle overfitting</li>
                    <li>Tried out Keras Tuner for hyperparameters</li>
                </ul>
            </p>
            <hr />
            <h3>Image Augmentation</h3>

            <p>
                Image augmentation is a technique to produce more data from an existing dataset. This helps the model to train on a wider range of Dataset and hence train more efficiently. <br>
                For the Image augmentation we have used Image Data Generator function from the Keras API in Tensorflow Library. The augmentation parameters are as given below <br>
                <img src="./images/aug.png" alt="Augmentation function"> <br>
                The function driving this generator, gets the image, turn it into an array, apply the augmentations and saves the image to a given directory using .flow function in the Image Data generator.
            </p>
            <hr />
            <h3>Image Segmentation</h3>
            As we observed earlier, using the original image dataset didnt gave us good results so we went with the Segmentation. What our aim was to extract out the image of the nucleus and/or cell, and train our model on the same. <br>
            We explored 2 image segmentation techniques, known as cell segmentation and nuclus segmentation. <br>
            <ul>
                <li>
                    Cell Segmentation: <br> 
                    The method summary is shown in the figure below. <br>
                    <img src="./images/cell.png" alt="Cell segmentation"> <br>
                    We began with taking the grayscale image and apply Adaptive Histogram Equalisation (AHE) on it. AHE is a technique that improves the contrast difference in the image. Then, different thresholding algorithms were tested on it. <br> Thresholding is a technique where once you select a threshold and make all values above it 1 and below it 0. This left us with binary image with black background containing white holes at the postition of the cells.
                    The thresholding algorithms that worked well were otsu isodata and mean thresholding. After that we used erosion for hole filling. Erosion is a technique which removes small black areas within larger white areas. We got good results for images with otsu thresholding only. So, we dropped the other thresholding algorithms. Then, finally using Image opening we were supposed to get rid of all white holes except for the one with WBC.
                    However, we were unable to achieve this. So, instead we went with Nucleus Segmentation.

                </li>

                <li>
                    Nucleus Segmentation: <br>
                    The method summary is show in the figure below. <br>
                    <img src="./images/nu2.png" alt="Nuclus Segmentation"> <br>
                    First the RGB values of image is converted into HSV values where, H = Hue, S = Saturation and V = Value. Then we separate the saturation image and treat it as a separate grayscale image. The reason for this is because the stained nucleus has a considerabely higher saturation value as compared to other regions in the image.
                    Then using a threshold, we were able to separate them. Three different algorithms gave us decent results. They were otsu, isodata and minimum thresholding. However, none of them worked perfectly for every image. So, we compared the results of each thresholding algorithm for random samples from the data and finally settled on otsu thresholding.
                    Now we have a binary image with white nucleus and black background. Then we multiplied its values with the original grayscale image to crop out the region containing the nucleus. 
                </li>
            </ul>
            <hr />
            <h3>Regularisation</h3>
            <p>
                Apart from augmentation and segmentation another technique used for solving the problem of overfitting is the technique called as Regularisation. <br>
                There are different types of regularisation known as L1 regularisation, L2 regularisation, Dropout regularisation etc. For CNN models, Dropout regularisation is widely used. <br> <br>
                <h4>Dropout Regularisation</h4>
                <img src="./images/drop.png" alt="Dropout Reg"> <br>
                <p>
                    As the name suggests, dropout regularisation layer "drops out" or ignores a given percentage of nodes in a layer while the training the epoch or it can be said that it temporarily removes those particular nodes from the network. The nodes are randomly selected in each iteration. The dropping percentage is again a hyperparameter that can be tuned, although in many papers it is said that, ideal dropout parameter for a hidden layer is between 0.5 to 0.8.
                </p>
            </p>
            <hr />

            <h2>Experimentations and different models : <a href="https://github.com/Girish0409/21cs460_group12/tree/main/Code%20and%20Data/Post%20Midway"> Link </a> to the codes </h2>
            
            <br>
            <ul>
                <li> <strong>Model 1</strong> <br>
                    <table>
                        <tbody>
                        <tr>
                        <td>&nbsp;Base Model</td>
                        <td>CNN Multilayered&nbsp;</td>
                        <td>&nbsp;Adam Optimiser</td>
                        <td>&nbsp;Softmax Activation</td>
                        <td>Training : 99.97 %&nbsp;</td>
                        <td>Testing : 27.51%</td>
                        
                        </tr>
                        </tbody>
                    </table> <br>
                    <img src="./images/m1.png" height="200" width="500"> <br>
                    As it can be observed from the confusion matrix, the model is a overfitted one. This model is used as a base model and all the hyperparameter tuning is done on this model                                         
                </li> <br>

                <li>
                    <strong>Model 2</strong>  <br>
                    <table>
                        <tbody>
                        <tr>
                        <td>&nbsp;Added image segmentation</td>
                        <td>CNN Multilayered&nbsp;</td>
                        <td>&nbsp;Adam Optimiser</td>
                        <td>&nbsp;Softmax Activation</td>
                        <td>Training : 98 %&nbsp;</td>
                        <td>Testing : 70 %</td>
                        
                        </tr>
                        </tbody>
                    </table> <br>
                    <img src="./images/m2.png" height="200" width="500"> <br>
                    This model with image segmentation worked a little better than the previous one, so we are on the right direction.

                </li> <br>

                <li>
                    <strong>Model 3</strong> <br>
                    <table>
                        <tbody>
                        <tr>
                        <td>&nbsp;Added a dropout layer</td>
                        <td>CNN Multilayered&nbsp;</td>
                        <td>&nbsp;Adam Optimiser</td>
                        <td>&nbsp;Softmax Activation</td>
                        <td>Training : 83 %&nbsp;</td>
                        <td>Testing : 74 %</td>
                        
                        </tr>
                        </tbody>
                    </table> <br>
                    <img src="./images/m3.png" height="200" width="500"> <br>
                    Adding a dropout regularisation layer made the training accuracy a bit better.
                </li> <br>

                <li>
                    <strong>Model 4</strong>
                    <table>
                        <tbody>
                        <tr>
                        <td>&nbsp;Image segmentation 2.0</td>
                        <td>CNN Multilayered&nbsp;</td>
                        <td>&nbsp;Adam Optimiser</td>
                        <td>&nbsp;Softmax Activation</td>
                        <td>Training : 98.74 %&nbsp;</td>
                        <td>Testing : 72 %</td>
                        
                        </tr>
                        </tbody>
                    </table> <br>
                    <img src="./images/m4.png" height="200" width="500"> <br>
                </li> <br>

                <li>
                    <strong>Model 5</strong>
                    <table>
                        <tbody>
                        <tr>
                        <td>&nbsp;Image Segmentation 2.0 with dropout layer</td>
                        <td>CNN Multilayered&nbsp;</td>
                        <td>&nbsp;Adam Optimiser</td>
                        <td>&nbsp;Softmax Activation</td>
                        <td>Training : 89.74 %&nbsp;</td>
                        <td>Testing : 77 %</td>
                        
                        </tr>
                        </tbody>
                    </table> <br>
                    <img src="./images/m5.png" height="200" width="500"> <br>
                    This is by far, the best model we were able to get by hyperparameter tuning.
                </li>


            </ul>
            <hr />
            <h3>Problems so far and possible fixes</h3>
            <ul>
                <li>Not able to train on the augmented data, can be solved by doing batchwise training of model on the augmented images</li>
                <li>Not clear image even after image segmentation, image needs to be cropped</li>
                <li>Model getting confused in Class 1 (Eosinophil) and class 4 (Nutrophil). Maybe due to similar structure of nucleus</li>
            </ul>
            <hr />
            
            <h2>Relevant Paper and Sources:</h2>
            <ul>
                <li>Sharma M., Bhave A., Janghel R.R. (2019) White Blood Cell Classification Using Convolutional Neural Network. In: Wang J., Reddy G., Prasad V., Reddy V. (eds) Soft Computing and Signal Processing. Advances in Intelligent Systems and Computing, vol 900. Springer, Singapore. https://doi.org/10.1007/978-981-13-3600-3_13. <a href="https://link.springer.com/chapter/10.1007/978-981-13-3600-3_13"> Link</a></li>
                <li>S. Manik, L. M. Saini and N. Vadera, "Counting and classification of white blood cell using Artificial Neural Network (ANN)," 2016 IEEE 1st International Conference on Power Electronics, Intelligent Control and Energy Systems (ICPEICES), 2016, pp. 1-5, doi: 10.1109/ICPEICES.2016.7853644. <a href="https://ieeexplore.ieee.org/abstract/document/7853644"> Link</a></li>
                <li>Image recognition using convolutional neural network combined with ensemble learning algorithm, Weilong Mo et al 2019 J. Phys.: Conf. Ser. 1237 022026 <a href="https://www.researchgate.net/publication/334426609_Image_recognition_using_convolutional_neural_network_combined_with_ensemble_learning_algorithm">Link</a></li>
                <li>Q. Li, W. Cai, X. Wang, Y. Zhou, D. D. Feng and M. Chen, "Medical image classification with convolutional neural network," 2014 13th International Conference on Control Automation Robotics & Vision (ICARCV), 2014, pp. 844-848, doi: 10.1109/ICARCV.2014.7064414. <a href="https://ieeexplore.ieee.org/document/7064414">Link</a></li>
                <li>Jiaohua Qin, Wenyan Pan, Xuyu Xiang, Yun Tan, Guimin Hou, A biological image classification method based on improved CNN, Ecological Informatics, Volume 58, 2020, 101093, ISSN 1574-9541, https://doi.org/10.1016/j.ecoinf.2020.101093. <a href="https://www.sciencedirect.com/science/article/abs/pii/S1574954120300431">Link</a></li>
            </ul>

        </div>
    </body>
    

</html>
